run_name: burn
log_path: ??? 
recurrent: False
use_custom: False 

env: ${procgen}
env_type: procgen

procgen:
  name:         'coinrun'
  train:
    env_name:   ${procgen.name}
    num_envs:   256 # buffer size == num_envs * ppo.n_steps = 65536
    num_levels: 100
    start_level: 0
    distribution_mode: 'hard'
    restrict_themes: False
    # render_mode: rgb_array -> this give 512x512 obs
  eval:
    env_name:    ${procgen.name}
    num_envs:    64
    num_levels:  100 # test on full distribution, following prior work 
    start_level: 100000 # to make it strictly disjoint
    distribution_mode: ${procgen.train.distribution_mode}
    restrict_themes: ${procgen.train.restrict_themes}
  
procgen_custom:
  name:         'coinrun'
  num_train_env:   256 
  num_eval_env: 64
  train:
    env_name:   ${procgen_custom.name} 
    max_trials: 1
    num_levels: 100
    start_level: 0
    distribution_mode: 'hard'
    restrict_themes: False
    is_train: True
  eval:
    env_name:    ${procgen_custom.name}
    max_trials:  ${procgen_custom.train.max_trials}
    num_levels:  50 # test on full distribution, following prior work 
    start_level: 100000 # set to ${env.train.num_levels}  to make it strictly disjoint
    distribution_mode: ${procgen_custom.train.distribution_mode}
    restrict_themes: ${procgen_custom.train.restrict_themes}
    is_train: False 

buffer_normalize: False 
subtract_mean: False
atari:
  env_ids: ['Asteroids', 'Breakout', 'BeamRider', 'Seaquest', ] # action space: Discrete(14) Discrete(4) Discrete(9) Discrete(18) 
  n_envs: 8 
  scale_rew: False
  use_subproc_vec_env: True 
  wrapper_kwargs:
    scale_reward: ${atari.scale_rew}
    terminal_on_life_loss: True
  eval_wrapper_kwargs:
    scale_reward: False
    terminal_on_life_loss: ${atari.wrapper_kwargs.terminal_on_life_loss}
  reset_action_space: -1  # for loading 4task agent to new task


vb: 0 
log_wb: True 
data_path: /home/mandi/stable-baselines3/meta-rl/log
load_run: ''
load_step: -1
eval_only: False
eval_steps: 1

learn:
  total_timesteps: 100_000_000 # openai used 200M for hard and 25M for easy
  # callback:        None 
  log_interval:    50
  eval_freq:       100 # this is w.r.t. iterations 
  n_eval_episodes: 6400
  tb_log_name:     "PPO"
  eval_log_path:   ???
  reset_num_timesteps: True 


cnn: ImpalaCNN # NatureCNN
defaults:
 - policy_cfg: DictObs


ppo:
  policy:        ${policy_cfg.name} 
  learning_rate: 5e-4
  n_steps:      256 
  batch_size:   2048
  n_epochs:     3
  gamma:        0.99
  gae_lambda:   0.95
  clip_range:      constant_0.2
  # clip_range_vf:   linear_30.0 # comment out for Atari
  ent_coef:        0.01
  vf_coef:         0.5
  max_grad_norm:   0.5
  use_sde:         False
  sde_sample_freq: -1
  target_kl:       0.01
  # tensorboard_log: None
  create_eval_env: True 
  policy_kwargs:   ${policy_cfg.kwargs}
  verbose:         ${vb}
  seed:            123
  device:          "cuda"
  _init_setup_model: True  
  recurrent:      ${policy_cfg.recurrent}
  debug_buffer: False 
  buffer_sample_strategy: "default" # "per_env"
  reptile_k: 0
  reptile_eps: 1

wandb:
  project:   'neurips22'
  entity:    mannndi
  job_type: 'train'
  group:    'Procgen'

hydra:
  run:
    dir: '/home/mandi/stable-baselines3/meta-rl/'
